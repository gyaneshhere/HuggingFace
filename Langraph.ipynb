{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141123,
     "status": "ok",
     "timestamp": 1743537759323,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "kw42Bek4DlG8",
    "outputId": "c81118f0-eac1-44b5-9691-c17b2059bbd0"
   },
   "outputs": [],
   "source": [
    "#pip install datasets langchain_community rank_bm25 langgraph langchain_huggingface duckduckgo-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1lYKRL0D20i"
   },
   "source": [
    "**Step 1: Load and Prepare the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roZ7dYnFDwoH"
   },
   "source": [
    "We will use the Hugging Face datasets library to load the dataset and convert it into a list of Document objects from the langchain.docstore.document module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "4886c6be08e3455e814f3190e782f211",
      "cc717fdac13f4c4eb917032d461064b2",
      "e71ecb7b2d184cc29cdc0d34fce5edda",
      "b919c33b983b4a699f18e47ac8826c48",
      "aea17bb0208a476687595c39c06d51bf",
      "f621062af5294abe85b2294c2897e44b",
      "e3a54c6c09f5439184ff7599ea96b7ea",
      "4558860478054ce2aeaafc29b436ac5a",
      "c485033063604097a74f0bebab444500",
      "3adff87cc859422fb749454556dfa2a1",
      "faa00473c2be4fe3951dcdb0150bb5be",
      "d16e9d571f1d4ab98e94d4253b1c57bc",
      "5e5d79f701294c5d90ddd0a13ea84e60",
      "01e62096977f4bf38faf1e7a196f20bd",
      "8e2198936606473fbd92b76d7f0b66b6",
      "6d159b2659ce43b0a6133d2c9f238bca",
      "e174b4d4506945f7ae5e155fbcb5233a",
      "3b5160171c19463383791b0397ee9227",
      "9f824e3bb85442b982f8a9804144bd2b",
      "51856aa59b57480db5ea089f4362b885",
      "498c9d665d8448aaa16d524c75f7b9c1",
      "96f6d2901a154e50975e34d842918009",
      "e4d035b739514eea99c56e40a7bf8dc4",
      "22c25407e0d342c2a99468c9a8a6ea94",
      "480f4f00897c4d3e9a8e43fdff92e006",
      "014f4af4e5f54b30b62a6566aee989f4",
      "4a8ff532b8674979b5fa455c8bfe013c",
      "02fb2f06a89045debe95972f68125e52",
      "55da0221908f44a7856385b9441f0f40",
      "e8a0b6ab511f4b3f958b75d05becc265",
      "80d40507667d4c7793792c1dbba9c66b",
      "e8fc1f8c23bb481da61dc11687927fef",
      "d219d7e6883a4343911233601efb391a"
     ]
    },
    "executionInfo": {
     "elapsed": 4446,
     "status": "ok",
     "timestamp": 1743538126469,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "ZIx9aigxDec2",
    "outputId": "fb0f17a0-751f-47c3-c5cd-4fbaa65b3221"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gyane\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\gyane\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gyane\\.cache\\huggingface\\hub\\datasets--agents-course--unit3-invitees. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|████████████████████████████████████████████████████| 3/3 [00:00<00:00, 397.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load the dataset\n",
    "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
    "\n",
    "# Convert dataset entries into Document objects\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"\\n\".join([\n",
    "            f\"Name: {guest['name']}\",\n",
    "            f\"Relation: {guest['relation']}\",\n",
    "            f\"Description: {guest['description']}\",\n",
    "            f\"Email: {guest['email']}\"\n",
    "        ]),\n",
    "        metadata={\"name\": guest[\"name\"]}\n",
    "    )\n",
    "    for guest in guest_dataset\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg7mbx0QEAvB"
   },
   "source": [
    "In the code above, we:\n",
    "\n",
    "Load the dataset\n",
    "\n",
    "Convert each guest entry into a Document object with formatted content\n",
    "\n",
    "Store the Document objects in a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVhIioFbEWSy"
   },
   "source": [
    "**Step 2: Create the Retriever Tool**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5PYXVG-Eg8Y"
   },
   "source": [
    "We will use the BM25Retriever from the langchain_community.retrievers module to create a retriever tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVb_ONtxFGsd"
   },
   "source": [
    "The BM25Retriever is a great starting point for retrieval, but for more advanced semantic search, you might consider using embedding-based retrievers like those from sentence-transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1743538130804,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "0SsXhlQlDzPV"
   },
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.tools import Tool\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "def extract_text(query: str) -> str:\n",
    "    \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "    results = bm25_retriever.invoke(query)\n",
    "    if results:\n",
    "        return \"\\n\\n\".join([doc.text for doc in results[:3]])\n",
    "    else:\n",
    "        return \"No matching guest information found.\"\n",
    "\n",
    "guest_info_tool = Tool(\n",
    "    name=\"guest_info_retriever\",\n",
    "    func=extract_text,\n",
    "    description=\"Retrieves detailed information about gala guests based on their name or relation.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bMEAS6SFOJG"
   },
   "source": [
    "Let’s understand this tool step-by-step.\n",
    "\n",
    "\n",
    "\n",
    "*   The name and description help the agent understand when and how to use this tool.\n",
    "*   The inputs define what parameters the tool expects (in this case, a search query)\n",
    "*   We’re using a BM25Retriever, which is a powerful text retrieval algorithm that doesn’t require embeddings\n",
    "*   The forward method processes the query and returns the most relevant guest information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otSDriBMF0lU"
   },
   "source": [
    "**Step 3: Integrate the Tool with Alfred**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 5464,
     "status": "error",
     "timestamp": 1743526982787,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "yBr_kIo0KNBF",
    "outputId": "763c98fc-1b45-4d60-d377-6c7531f1b836"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gyane\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gyane\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-Coder-32B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGraphRecursionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m alfred = builder.compile()\n\u001b[32m     46\u001b[39m messages = [HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mTell me about our guest named \u001b[39m\u001b[33m'\u001b[39m\u001b[33mLady Ada Lovelace\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m response = \u001b[43malfred\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🎩 Alfred\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms Response:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(messages[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m].content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2714\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2712\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2713\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m2714\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2715\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2716\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2718\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2719\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2373\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loop.status == \u001b[33m\"\u001b[39m\u001b[33mout_of_steps\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2365\u001b[39m     msg = create_error_message(\n\u001b[32m   2366\u001b[39m         message=(\n\u001b[32m   2367\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mrecursion_limit\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m reached \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2371\u001b[39m         error_code=ErrorCode.GRAPH_RECURSION_LIMIT,\n\u001b[32m   2372\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2373\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[32m   2374\u001b[39m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[32m   2375\u001b[39m run_manager.on_chain_end(loop.output)\n",
      "\u001b[31mGraphRecursionError\u001b[39m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "# Generate the chat interface, including the tools\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    huggingfacehub_api_token=\"token-goes-here\",\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "tools = [guest_info_tool]\n",
    "chat_with_tools = chat.bind_tools(tools)\n",
    "\n",
    "# Generate the AgentState and Agent graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "## The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "alfred = builder.compile()\n",
    "\n",
    "messages = [HumanMessage(content=\"Tell me about our guest named 'Lady Ada Lovelace'.\")]\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(messages['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQvLwHFVcub9"
   },
   "source": [
    "**Give Your Agent Access to the Web**\n",
    "\n",
    "Remember that we want Alfred to establish his presence as a true renaissance host, with a deep knowledge of the world.\n",
    "\n",
    "To do so, we need to make sure that Alfred has access to the latest news and information about the world.\n",
    "\n",
    "Let’s start by creating a web search tool for Alfred!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1070,
     "status": "ok",
     "timestamp": 1743529126816,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "P_aucQ9_bdd4",
    "outputId": "0e7f350a-6c82-4c59-96ef-145902673ea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emmanuel Macron is a French banker and politician who was elected president of France in 2017. Macron was the first person in the history of the Fifth Republic to win the presidency without the backing of either the Socialists or the Gaullists, and he was France's youngest head of state since Napoleon. Find out who the current president of France is, his political career, his actions, and his impact on the country. Stay informed about French news and presidential decisions. The current President of France is Emmanuel Macron, who has held office since being elected in the 2017 French Presidential Election. The Prime Minister of France is the leader of government and holds the power to manage the numerous public agencies based around the nation. PARIS (AP) — French President Emmanuel Macron vowed Thursday to stay in office until the end of his term, due in 2027, and announced that he will name a new prime minister within days following ... France has a semi-presidential system of government, offering almost a balanced power sharing between the president and the prime minister.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "results = search_tool.invoke(\"Who's the current President of France?\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHtQQ4gHclC1"
   },
   "source": [
    "**Creating a Custom Tool for Weather Information to Schedule the Fireworks**\n",
    "\n",
    "The perfect gala would have fireworks over a clear sky, se need to make sure the fireworks are not cancelled due to bad weather.\n",
    "\n",
    "Let’s create a custom tool that can be used to call an external weather API and get the weather information for a given location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1743538211776,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "2VQc1Lxub3Zx"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "import random\n",
    "\n",
    "def get_weather_info(location: str) -> str:\n",
    "    \"\"\"Fetches dummy weather information for a given location.\"\"\"\n",
    "    # Dummy weather data\n",
    "    weather_conditions = [\n",
    "        {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
    "        {\"condition\": \"Clear\", \"temp_c\": 25},\n",
    "        {\"condition\": \"Windy\", \"temp_c\": 20}\n",
    "    ]\n",
    "    # Randomly select a weather condition\n",
    "    data = random.choice(weather_conditions)\n",
    "    return f\"Weather in {location}: {data['condition']}, {data['temp_c']}°C\"\n",
    "\n",
    "# Initialize the tool\n",
    "weather_info_tool = Tool(\n",
    "    name=\"get_weather_info\",\n",
    "    func=get_weather_info,\n",
    "    description=\"Fetches dummy weather information for a given location.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipjRQPd-ccdw"
   },
   "source": [
    "**Creating a Hub Stats Tool for Influential AI Builders**\n",
    "\n",
    "\n",
    "In attendance at the gala are the who’s who of AI builders. Alfred wants to impress them by discussing their most popular models, datasets, and spaces. We’ll create a tool to fetch model statistics from the Hugging Face Hub based on a username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3736,
     "status": "ok",
     "timestamp": 1743538217546,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "kMHlxJrcb7Fu",
    "outputId": "6d07c6bb-a394-4126-b66f-ceee3672d190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most downloaded model by facebook is facebook/esmfold_v1 with 16,832,501 downloads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gyane\\AppData\\Local\\Temp\\ipykernel_6160\\3056453286.py:26: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(hub_stats_tool(\"facebook\")) # Example: Get the most downloaded model by Facebook\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import Tool\n",
    "from huggingface_hub import list_models\n",
    "\n",
    "def get_hub_stats(author: str) -> str:\n",
    "    \"\"\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\"\"\n",
    "    try:\n",
    "        # List models from the specified author, sorted by downloads\n",
    "        models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
    "\n",
    "        if models:\n",
    "            model = models[0]\n",
    "            return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"\n",
    "        else:\n",
    "            return f\"No models found for author {author}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching models for {author}: {str(e)}\"\n",
    "\n",
    "# Initialize the tool\n",
    "hub_stats_tool = Tool(\n",
    "    name=\"get_hub_stats\",\n",
    "    func=get_hub_stats,\n",
    "    description=\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "print(hub_stats_tool(\"facebook\")) # Example: Get the most downloaded model by Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsyBqEP0cShc"
   },
   "source": [
    "**Integrating Tools with Alfred**\n",
    "\n",
    "Now that we have all the tools, let’s integrate them into Alfred’s agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10953,
     "status": "ok",
     "timestamp": 1743529227188,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "Mdtbz0Ltb--t",
    "outputId": "5679ec72-2cd0-4a5d-993b-348293f054b9"
   },
   "outputs": [
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGraphRecursionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m alfred = builder.compile()\n\u001b[32m     46\u001b[39m messages = [HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWho is Facebook and what\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms their most popular model?\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m response = \u001b[43malfred\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🎩 Alfred\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms Response:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(response[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m].content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2714\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2712\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2713\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m2714\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2715\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2716\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2718\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2719\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2373\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loop.status == \u001b[33m\"\u001b[39m\u001b[33mout_of_steps\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2365\u001b[39m     msg = create_error_message(\n\u001b[32m   2366\u001b[39m         message=(\n\u001b[32m   2367\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mrecursion_limit\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m reached \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2371\u001b[39m         error_code=ErrorCode.GRAPH_RECURSION_LIMIT,\n\u001b[32m   2372\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2373\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[32m   2374\u001b[39m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[32m   2375\u001b[39m run_manager.on_chain_end(loop.output)\n",
      "\u001b[31mGraphRecursionError\u001b[39m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "# Generate the chat interface, including the tools\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    huggingfacehub_api_token=\"hf_token goes here\",\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "tools = [search_tool, weather_info_tool, hub_stats_tool]\n",
    "chat_with_tools = chat.bind_tools(tools)\n",
    "\n",
    "# Generate the AgentState and Agent graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "## The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "alfred = builder.compile()\n",
    "\n",
    "messages = [HumanMessage(content=\"Who is Facebook and what's their most popular model?\")]\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1743538041933,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "vh3Omqfi8Ath"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 891,
     "status": "ok",
     "timestamp": 1743538225223,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "kip_lREG8css"
   },
   "outputs": [],
   "source": [
    "# Initialize the web search tool\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "# Load the guest dataset and initialize the guest info tool\n",
    "guest_info_tool = guest_info_tool\n",
    "\n",
    "# Generate the chat interface, including the tools\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    huggingfacehub_api_token=\"hf_token-goes-here\",\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "tools = [guest_info_tool, search_tool, weather_info_tool, hub_stats_tool]\n",
    "chat_with_tools = chat.bind_tools(tools)\n",
    "\n",
    "# Generate the AgentState and Agent graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "## The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "alfred = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12812,
     "status": "ok",
     "timestamp": 1743538260133,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "bMqPdP6a-kl_",
    "outputId": "c7a18b4a-2b2c-4c1f-924a-c9867e56a573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎩 Alfred's Response:\n",
      "Ada Lovelace, born Augusta Ada Byron on December 10, 1815, was the only legitimate child of the famous poet, Lord Byron, and Annabella Milbanke. Her mother had a strong interest in mathematics and logic, which led her to encourage Ada to pursue these subjects. This early exposure to mathematics likely played a significant role in shaping Ada's academic and professional life.\n",
      "\n",
      "In the 1840s, Ada met Charles Babbage, a mathematician renowned for his work on mechanical computing devices, most famously the Analytical Engine, which was an early conceptualization for a computer. The Analytical Engine could perform complex calculations, store data, and be \"programmed\" with punched cards, much like early computers did a century later.\n",
      "\n",
      "Lovelace collaborated with Babbage, translating an article from French about the Analytical Engine into English and appending her own detailed comments, including a set of notes that are now regarded as the first algorithm intended to be processed by a machine. These notes, written in 1843, describe a method for computing the Bernoulli numbers, making Ada Lovelace the world's first computer programmer.\n",
      "\n",
      "Her contributions to computer science have been acknowledged through numerous initiatives. Ada Lovelace Day, now celebrated on the second Tuesday in October, honors the achievements of women in science, technology, engineering, and mathematics (STEM). The day is named after Ada Lovelace and includes talks, networking events, and other activities to inspire and raise the profile of women in STEM fields.\n",
      "\n",
      "Ada Lovelace's visionary work and profound understanding of the potential of early computing have ensured her place in both the history of mathematics and in the annals of computing.\n"
     ]
    }
   ],
   "source": [
    "response = alfred.invoke({\"messages\": \"Tell me about 'Lady Ada Lovelace'\"})\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5574,
     "status": "ok",
     "timestamp": 1743538273254,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "Naydz2T7-por",
    "outputId": "a81fdda6-14bf-4c4e-a1d4-8133dc041956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎩 Alfred's Response:\n",
      "Given that the weather in Paris tonight is expected to be windy with a temperature of 20°C, it might not be the most ideal conditions for a fireworks display. The wind could potentially scatter the fireworks, making the show less impressive and harder to enjoy. You might want to consider rescheduling the fireworks display to a night with clearer and calmer conditions for the best viewing experience.\n"
     ]
    }
   ],
   "source": [
    "response = alfred.invoke({\"messages\": \"What's the weather like in Paris tonight? Will it be suitable for our fireworks display?\"})\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27090,
     "status": "ok",
     "timestamp": 1743538319009,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "PKmie6Py-vlN",
    "outputId": "c21aea62-306e-4e6f-9f52-cce6198410cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎩 Alfred's Response:\n",
      "It seems there might have been a misunderstanding. According to the information available, Qwen is a large language model created by Alibaba Cloud, and the specific model details you were inquiring about may not have been correctly referenced.\n",
      "\n",
      "As of my last update, Qwen includes a variety of models optimized for different tasks, such as text generation, translation, and more. If \"Qwen/Qwen2.5-7B-Instruct\" is a specific variant you're referring to, it seems to be very popular, with over 2 million downloads, as indicated in your initial message.\n",
      "\n",
      "However, if you need specific information about the most popular Qwen model, I recommend checking the latest releases and download statistics on the official platform or repository where Qwen models are hosted, such as Hugging Face Transformers or Alibaba Cloud's repository.\n",
      "\n",
      "If you have more details or another specific model in mind, feel free to provide them, and I can try to provide more accurate information!\n"
     ]
    }
   ],
   "source": [
    "response = alfred.invoke({\"messages\": \"One of our guests is from Qwen. What can you tell me about their most popular model?\"})\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5789,
     "status": "ok",
     "timestamp": 1743538564747,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "8Qht8gG0-1UM",
    "outputId": "8655ffa8-c304-42de-c0d5-ddb9fbfdae57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎩 Alfred's Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = alfred.invoke({\"messages\":\"I need to speak with 'Dr. Nikola Tesla' about recent advancements in wireless energy. Can you help me prepare for this conversation?\"})\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41718,
     "status": "ok",
     "timestamp": 1743541829564,
     "user": {
      "displayName": "Gyanesh Kumar",
      "userId": "09704110288993269691"
     },
     "user_tz": 300
    },
    "id": "-GgbQYjR_JS8",
    "outputId": "826ecfb9-f90e-4652-92a9-50684f8ab7db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎩 Alfred's Response:\n",
      "Ada Lovelace, born Augusta Ada Byron on December 10, 1815, and later known as the Countess of Lovelace, is celebrated as one of the first computer programmers. She was the daughter of the famous poet Lord Byron and Annabella Milbanke, a woman who had a strong interest in mathematics and logic, which likely influenced Ada's early life and education.\n",
      "\n",
      "Ada's mother feared that Ada would inherit her father's tumultuous nature and focused on scientific, mathematical, and musical education to steer her away from poetry. Under her mother's supervision, Ada received rigorous tutoring in mathematics, a subject she excelled in. Her aptitude for numbers caught the attention of Charles Babbage, a leading English mathematician and inventor. Babbage was impressed by Ada’s intellect and invited her to attend his lectures at the University of London.\n",
      "\n",
      "Babbage was working on the Analytical Engine, a mechanical device designed to perform complex mathematical calculations. Many considered it the precursor to modern computers. Ada saw potential beyond mere calculations and understood the machine’s capacity to handle a broader range of processes, not just numbers. She wrote what is often considered the first algorithm intended to be processed by a machine on the Analytical Engine, a significant step in the history of computer science.\n",
      "\n",
      "Ada Lovelace passed away on November 27, 1852, due to uterine cancer. Her contributions to mathematics and computer science were recognized much later, but today, she is remembered as a pioneer who foresaw the future possibilities of computing long before it existed in its modern form.\n",
      "\n",
      "Regarding your personal connection, as a descendant of Ada Lovelace, you are part of a lineage that has made significant contributions to the fields of science, technology, and mathematics. Her legacy continues to inspire, particularly in encouraging women to pursue careers in these fields. Ada Lovelace Day, observed on the second Tuesday of October, is dedicated to celebrating the achievements of women in science, technology, engineering, and mathematics, a day when your heritage and her achievements are notably recognized.\n",
      "\n",
      "🎩 Alfred's Response:\n",
      "It seems there is some confusion here. Ada Lovelace, born Augusta Ada Byron in 1815 and passing away in 1852, cannot be working on any current projects as she lived well before the modern era of technology and computing. The references you provided are related to organizations, events, and research initiatives named in her honor, but they are not about Ada Lovelace herself.\n",
      "\n",
      "Here's a summary of the information related to Ada Lovelace and the projects and initiatives named after her:\n",
      "\n",
      "1. **Ada Lovelace Institute**: This is an independent research institute in the UK with a mission to ensure data and AI work for people and society. They conduct research, policy work, and host events related to AI and data ethics. For example, they have a research project focused on understanding public compute strategies and the role of AI in public services.\n",
      "\n",
      "2. **Ada Lovelace Day**: This is an annual global celebration of women in STEM (Science, Technology, Engineering, and Mathematics) fields. It aims to raise the profile of women in these areas and encourage more women to pursue careers in these fields.\n",
      "\n",
      "3. **Ada Lovelace Programme**: Established in 2023, this program focuses on developing cooperation in the thematic area of \"Humans in the Digital Age\" and strengthening cross-disciplinary research networking efforts. It includes several research projects.\n",
      "\n",
      "While these projects and initiatives are named after Ada Lovelace and are inspired by her pioneering work in computing, they are distinct from any work she might have done during her lifetime. They are modern efforts to honor her legacy and contribute to the fields of technology, data ethics, and STEM.\n"
     ]
    }
   ],
   "source": [
    "# First interaction\n",
    "response = alfred.invoke({\"messages\": [HumanMessage(content=\"Tell me about 'Lady Ada Lovelace'. What's her background and how is she related to me?\")]})\n",
    "\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)\n",
    "print()\n",
    "\n",
    "# Second interaction (referencing the first)\n",
    "response = alfred.invoke({\"messages\": response[\"messages\"] + [HumanMessage(content=\"What projects is she currently working on?\")]})\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "/v2/external/notebooks/intro.ipynb",
     "timestamp": 1743523572252
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "014f4af4e5f54b30b62a6566aee989f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8fc1f8c23bb481da61dc11687927fef",
      "placeholder": "​",
      "style": "IPY_MODEL_d219d7e6883a4343911233601efb391a",
      "value": " 3/3 [00:00&lt;00:00, 47.88 examples/s]"
     }
    },
    "01e62096977f4bf38faf1e7a196f20bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f824e3bb85442b982f8a9804144bd2b",
      "max": 3321,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51856aa59b57480db5ea089f4362b885",
      "value": 3321
     }
    },
    "02fb2f06a89045debe95972f68125e52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22c25407e0d342c2a99468c9a8a6ea94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02fb2f06a89045debe95972f68125e52",
      "placeholder": "​",
      "style": "IPY_MODEL_55da0221908f44a7856385b9441f0f40",
      "value": "Generating train split: 100%"
     }
    },
    "3adff87cc859422fb749454556dfa2a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b5160171c19463383791b0397ee9227": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4558860478054ce2aeaafc29b436ac5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "480f4f00897c4d3e9a8e43fdff92e006": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8a0b6ab511f4b3f958b75d05becc265",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_80d40507667d4c7793792c1dbba9c66b",
      "value": 3
     }
    },
    "4886c6be08e3455e814f3190e782f211": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc717fdac13f4c4eb917032d461064b2",
       "IPY_MODEL_e71ecb7b2d184cc29cdc0d34fce5edda",
       "IPY_MODEL_b919c33b983b4a699f18e47ac8826c48"
      ],
      "layout": "IPY_MODEL_aea17bb0208a476687595c39c06d51bf"
     }
    },
    "498c9d665d8448aaa16d524c75f7b9c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a8ff532b8674979b5fa455c8bfe013c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51856aa59b57480db5ea089f4362b885": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "55da0221908f44a7856385b9441f0f40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e5d79f701294c5d90ddd0a13ea84e60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e174b4d4506945f7ae5e155fbcb5233a",
      "placeholder": "​",
      "style": "IPY_MODEL_3b5160171c19463383791b0397ee9227",
      "value": "train-00000-of-00001.parquet: 100%"
     }
    },
    "6d159b2659ce43b0a6133d2c9f238bca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80d40507667d4c7793792c1dbba9c66b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8e2198936606473fbd92b76d7f0b66b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_498c9d665d8448aaa16d524c75f7b9c1",
      "placeholder": "​",
      "style": "IPY_MODEL_96f6d2901a154e50975e34d842918009",
      "value": " 3.32k/3.32k [00:00&lt;00:00, 188kB/s]"
     }
    },
    "96f6d2901a154e50975e34d842918009": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f824e3bb85442b982f8a9804144bd2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aea17bb0208a476687595c39c06d51bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b919c33b983b4a699f18e47ac8826c48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3adff87cc859422fb749454556dfa2a1",
      "placeholder": "​",
      "style": "IPY_MODEL_faa00473c2be4fe3951dcdb0150bb5be",
      "value": " 371/371 [00:00&lt;00:00, 12.6kB/s]"
     }
    },
    "c485033063604097a74f0bebab444500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cc717fdac13f4c4eb917032d461064b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f621062af5294abe85b2294c2897e44b",
      "placeholder": "​",
      "style": "IPY_MODEL_e3a54c6c09f5439184ff7599ea96b7ea",
      "value": "README.md: 100%"
     }
    },
    "d16e9d571f1d4ab98e94d4253b1c57bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e5d79f701294c5d90ddd0a13ea84e60",
       "IPY_MODEL_01e62096977f4bf38faf1e7a196f20bd",
       "IPY_MODEL_8e2198936606473fbd92b76d7f0b66b6"
      ],
      "layout": "IPY_MODEL_6d159b2659ce43b0a6133d2c9f238bca"
     }
    },
    "d219d7e6883a4343911233601efb391a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e174b4d4506945f7ae5e155fbcb5233a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3a54c6c09f5439184ff7599ea96b7ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4d035b739514eea99c56e40a7bf8dc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_22c25407e0d342c2a99468c9a8a6ea94",
       "IPY_MODEL_480f4f00897c4d3e9a8e43fdff92e006",
       "IPY_MODEL_014f4af4e5f54b30b62a6566aee989f4"
      ],
      "layout": "IPY_MODEL_4a8ff532b8674979b5fa455c8bfe013c"
     }
    },
    "e71ecb7b2d184cc29cdc0d34fce5edda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4558860478054ce2aeaafc29b436ac5a",
      "max": 371,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c485033063604097a74f0bebab444500",
      "value": 371
     }
    },
    "e8a0b6ab511f4b3f958b75d05becc265": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8fc1f8c23bb481da61dc11687927fef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f621062af5294abe85b2294c2897e44b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "faa00473c2be4fe3951dcdb0150bb5be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
